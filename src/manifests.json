{"k8s/ci/database.yaml":{"isBinary":false,"fileName":"k8s/ci/database.yaml","originalFileName":"k8s/ci/database.yaml","content":"apiVersion: ydb.tech/v1alpha1\nkind: Database\nmetadata:\n  name: database-sample\nspec:\n  monitoring:\n    enabled: true\n    interval: 3s\n  additionalLabels:\n    ydb-cluster: slo-storage\n    ydb-cluster-monitor: slo-storage\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchExpressions:\n              - key: ydb-cluster\n                operator: In\n                values:\n                  - slo-storage\n          topologyKey: 'kubernetes.io/hostname'\n  image:\n    name: cr.yandex/crptqonuodf51kdj7a7d/ydb:${{VERSION}}\n  nodes: 6\n  resources:\n    containerResources:\n      limits:\n        cpu: 1\n    storageUnits:\n      - count: 1\n        unitKind: ssd\n  storageClusterRef:\n    name: storage-sample\n"},"k8s/ci/storage.yaml":{"isBinary":false,"fileName":"k8s/ci/storage.yaml","originalFileName":"k8s/ci/storage.yaml","content":"apiVersion: ydb.tech/v1alpha1\nkind: Storage\nmetadata:\n  name: storage-sample\nspec:\n  monitoring:\n    enabled: true\n    interval: 3s\n  additionalLabels:\n    ydb-cluster: slo-storage\n    ydb-cluster-monitor: slo-storage\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchExpressions:\n              - key: ydb-cluster\n                operator: In\n                values:\n                  - slo-storage\n          topologyKey: 'kubernetes.io/hostname'\n  dataStore:\n    - volumeMode: Block\n      storageClassName: yc-network-ssd-nonreplicated\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 93Gi\n  image:\n    name: cr.yandex/crptqonuodf51kdj7a7d/ydb:${{VERSION}}\n  nodes: 9\n  erasure: mirror-3-dc\n  configuration: |-\n    static_erasure: mirror-3-dc\n    host_configs:\n    - drive:\n      - path: /dev/kikimr_ssd_00\n        type: SSD\n      host_config_id: 1\n    grpc_config:\n      port: 2135\n    domains_config:\n      domain:\n        - name: root\n          storage_pool_types:\n            - kind: ssd\n              pool_config:\n                box_id: 1\n                erasure_species: mirror-3-dc\n                kind: ssd\n                pdisk_filter:\n                  - property:\n                      - type: SSD\n                vdisk_kind: Default\n      state_storage:\n        - ring:\n            node: [ 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\n            nto_select: 5\n          ssid: 1\n    actor_system_config:\n      batch_executor: 2\n      io_executor: 3\n      executor:\n        - name: System\n          spin_threshold: 0\n          threads: 2\n          type: BASIC\n        - name: User\n          spin_threshold: 0\n          threads: 3\n          type: BASIC\n        - name: Batch\n          spin_threshold: 0\n          threads: 2\n          type: BASIC\n        - name: IO\n          threads: 1\n          time_per_mailbox_micro_secs: 100\n          type: IO\n        - name: IC\n          spin_threshold: 10\n          threads: 1\n          time_per_mailbox_micro_secs: 100\n          type: BASIC\n      scheduler:\n        progress_threshold: 10000\n        resolution: 256\n        spin_threshold: 0\n      service_executor:\n        - executor_id: 4\n          service_name: Interconnect\n    blob_storage_config:\n      service_set:\n        availability_domains: 1\n        groups:\n          - erasure_species: mirror-3-dc\n            group_id: 0\n            group_generation: 1\n            rings:\n              - fail_domains:\n                - vdisk_locations:\n                  - node_id: 1\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 2\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 3\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n              - fail_domains:\n                - vdisk_locations:\n                  - node_id: 4\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 5\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 6\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n              - fail_domains:\n                - vdisk_locations:\n                  - node_id: 7\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 8\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n                - vdisk_locations:\n                  - node_id: 9\n                    pdisk_category: SSD\n                    path: /dev/kikimr_ssd_00\n    channel_profile_config:\n      profile:\n        - channel:\n            - erasure_species: mirror-3-dc\n              pdisk_category: 1\n              storage_pool_kind: ssd\n            - erasure_species: mirror-3-dc\n              pdisk_category: 1\n              storage_pool_kind: ssd\n            - erasure_species: mirror-3-dc\n              pdisk_category: 1\n              storage_pool_kind: ssd\n          profile_id: 0\n"},"k8s/ci/workload.yaml":{"isBinary":false,"fileName":"k8s/ci/workload.yaml","originalFileName":"k8s/ci/workload.yaml","content":"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ${{LANGUAGE_ID}}-wl-${{COMMAND}}\nspec:\n  ttlSecondsAfterFinished: 120\n  backoffLimit: 0\n  template:\n    metadata:\n      name: ${{LANGUAGE_ID}}-wl-${{COMMAND}}\n    spec:\n      containers:\n        - name: ${{LANGUAGE_ID}}-wl-${{COMMAND}}\n          image: ${{DOCKER_IMAGE}}:latest\n          args:\n            - '${{COMMAND}}'\n            - ${{ARGS}}\n      restartPolicy: Never\n"}}